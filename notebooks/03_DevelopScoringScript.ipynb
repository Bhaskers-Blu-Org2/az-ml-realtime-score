{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Scoring Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will develop the scoring script and test it locally. We will use the scoring script to create the \n",
    "web service that will call the model for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from azure_utils.utilities import text_to_json, get_auth\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.workspace import Workspace\n",
    "from dotenv import get_key, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the model registered earlier and download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'question_match_model'\n",
    "model_version = int(get_key(env_path, 'model_version'))\n",
    "model = Model(ws, name=model_name, version=model_version)\n",
    "print(model.name, model.version, model.url, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download(target_dir=\".\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scoring Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the writefile magic to write the contents of the below cell to `score.py` which includes the  `init` and `run` \n",
    "functions required by AML.\n",
    "- The init() function typically loads the model into a global object.\n",
    "- The run(input_data) function uses the model to predict a value based on the input_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from duplicate_model import DuplicateModel\n",
    "import logging\n",
    "import timeit as t\n",
    "\n",
    "def init():\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    global model\n",
    "    model_path = \"model.pkl\"\n",
    "    questions_path = \"./data_folder/questions.tsv\"\n",
    "    start = t.default_timer()\n",
    "    model = DuplicateModel(model_path, questions_path)\n",
    "    end = t.default_timer()\n",
    "    loadTimeMsg = \"Model loading time: {0} ms\".format(round((end-start)*1000, 2))\n",
    "    logger.info(loadTimeMsg)\n",
    "\n",
    "\n",
    "def run(body):\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    json_load_text = json.loads(body)\n",
    "    text_to_score = json_load_text[\"input\"]\n",
    "    start = t.default_timer()\n",
    "    resp = model.score(text_to_score)\n",
    "    end = t.default_timer()\n",
    "    logger.info(\"Prediction took {0} ms\".format(round((end-start)*1000, 2)))\n",
    "    return json.dumps(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test by running the score.py which will bring the imports and functions into the context of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%run score.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes_test_path = './data_folder/dupes_test.tsv'\n",
    "dupes_test = pd.read_csv(dupes_test_path, sep='\\t', encoding='latin1')\n",
    "text_to_score = dupes_test.iloc[0, 4]\n",
    "print(text_to_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the init() to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the question text to json format and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text = text_to_json(text_to_score)\n",
    "r = run(json_text)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to [creating the docker image which we will deploy](04_CreateImage.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployAML]",
   "language": "python",
   "name": "conda-env-MLAKSDeployAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}